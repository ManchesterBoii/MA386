{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# MA486 Project 4: Predicting RASP Graduation\n",
    "### Andrew Efaw, Vedin Fowler, Huram-Abi Nzia Yotchoum, Radford Swent\n",
    "\n",
    "Cover page and works cited attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ANN import weight_bias_init_Nlayers, forward_propagation, N_layer_FW_Prop, N_layer_BWD_Prop, N_layer_weights_update, train_NN, model_predict, train_NN_final_model\n",
    "from datetime import timedelta \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "First, we start by formatting the data. These functions will allow us to format the data before our neural network processes it, and will allow us to transform LTC Clark's data into a useable format. Of note, we filled missing data in 2 ways:\n",
    "    \n",
    "    1) If the data was for a physical event, we filled with group mean of gender and graduation\n",
    "    \n",
    "    2) Otherwise, we filled with the graduation group mean\n",
    "    \n",
    "    \n",
    "We also standardized our data so that the neural network has an easier time understanding it. We encoded sex numerically, but otherwise chose to drop categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "RASP_data = pd.read_csv(\"train_data.csv\")\n",
    "\n",
    "def seconder(x):\n",
    "    \"\"\"\n",
    "    A function that accepts a column of data that is measured in mins:seconds:milliseconds (eg 2:34:56)\n",
    "        Inputs: x: a list or column of a pandas dataframe with time measurements\n",
    "        Outputs: returns a list of all time entries in seconds\n",
    "    \"\"\"\n",
    "    time  =  str(x).split(\":\")\n",
    "    if len(time) == 3:\n",
    "            mins, secs, mil = time\n",
    "            td = timedelta(minutes = float(mins), seconds = float(secs), milliseconds = float(mil))\n",
    "            return td.total_seconds()\n",
    "    elif len(time) == 2:\n",
    "            mins, secs = time\n",
    "            td = timedelta(minutes = float(mins), seconds = float(secs))\n",
    "            return td.total_seconds()\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def format_data(df):\n",
    "    \"\"\"\n",
    "    Takes dataframe in the format we are given and formats it to work with our functions\n",
    "    \"\"\"\n",
    "    if not isinstance(df[\"Run\"][0], np.float64):\n",
    "        df[\"Run\"] = df[\"Run\"].apply(seconder)\n",
    "    columns = list(df.columns)\n",
    "    for col in columns:\n",
    "        #numerically encode sex\n",
    "        if col == \"SEX\":\n",
    "            df.loc[df['SEX'] == \"F\", 'SEX_ENC'] =  1\n",
    "            df.loc[df['SEX'] == \"M\", 'SEX_ENC'] = 0\n",
    "        #Here we delete the non numerical columns\n",
    "        if df[f'{col}'].dtypes == np.dtype('O'):\n",
    "            del df[f'{col}']\n",
    "    #Assume all cases where we do not know the sex that the candidate is male\n",
    "    df['SEX_ENC'] = df['SEX_ENC'].fillna(0)\n",
    "    #fill NA values for physical events with the group mean for sex and whether they passed/not\n",
    "    df[['SEX_ENC', 'Graduate', 'Pushup','Situp','Run','Chinup']] = df[['SEX_ENC', 'Graduate', 'Pushup', 'Situp', 'Run', 'Chinup']].fillna(df[['SEX_ENC', 'Graduate', 'Pushup', 'Situp', 'Run', 'Chinup']].groupby(['SEX_ENC', 'Graduate']).transform('mean'))\n",
    "    #for everything else, just fill with general mean\n",
    "    df = df.fillna(df.groupby('Graduate').transform('mean'))\n",
    "    #These are the columns that will be used in our model, in proper order\n",
    "    cols = ['SEX_ENC', 'Married', 'Airborne', 'Age', 'Education',  'GT Score', 'Pushup', 'Situp', 'Run', 'Chinup', 'Sick Call', 'Information', 'Comprehension', 'Arithmetic', 'Similarities', 'Vocabulary', 'DS', 'PC', 'Spatial', 'PA', 'OA', 'V', 'P', 'FS', 'VRINr', 'TRINr', 'Fr', 'Fpr', 'Fs', 'FBSr', 'RBS', 'Lr', 'Kr', 'EID', 'THD', 'BXD', 'RCd', 'RC1', 'RC2', 'RC3', 'RC4', 'RC6', 'RC7', 'RC8', 'RC9', 'MLS', 'HPC', 'NUC', 'GIC', 'SUI', 'HLP', 'SFD', 'NFC', 'COG', 'STW', 'AXY', 'ANP', 'BRF', 'MSF', 'JCP', 'SUB', 'AGG', 'ACT', 'FML', 'IPP', 'SAV', 'SHY', 'DSF', 'AES', 'MEC', 'AGGRr', 'PSYCr', 'DISCr', 'NEGEr', 'INTRr', 'CS', 'PCT T', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'S10', 'S11', 'S12', 'S13', 'S14', 'S15', 'MTS1', 'MTS2', 'MTS3', 'MTS4', 'MTS5', 'MTS6', 'MTS7', 'MTS8', 'MTS9', 'MTS10', 'MTS11', 'MTS12', 'MTS13', 'MTS14', 'MTS15', 'FTS1', 'FTS2', 'FTS3', 'FTS4', 'FTS5', 'FTS6', 'FTS7', 'FTS8', 'FTS9', 'FTS10', 'FTS11', 'FTS12', 'FTS13', 'FTS14', 'FTS15', 'TSC1', 'TSC2', 'TSC3', 'TSC4', 'TSC5', 'TSC6', 'TSC7', 'TSC8', 'TSC9', 'TSC10','TSC11', 'TSC12', 'TSC13', 'TSC14', 'TSC15', 'MPC1', 'MPC2', 'MPC3', 'MPC4','MPC5', 'MPC6', 'MPC7', 'MPC8', 'MPC9', 'MPC10', 'MPC11', 'MPC12', 'MPC13', 'MPC14', 'MPC15', 'FPC1', 'FPC2', 'FPC3', 'FPC4', 'FPC5', 'FPC6', 'FPC7', 'FPC8', 'FPC9', 'FPC10', 'FPC11','FPC12', 'FPC13', 'FPC14', 'FPC15', 'PCT1', 'PCT2', 'PCT3', 'PCT4', 'PCT5', 'PCT6', 'PCT7', 'PCT8', 'PCT9', 'PCT10', 'PCT11', 'PCT12', 'PCT13', 'PCT14', 'PCT15', 'RCL1', 'RCL2', 'RCL3', 'RCL4', 'RCL5', 'MCL1', 'MCL2', 'MCL3', 'MCL4', 'MCL5', 'FCL1', 'FCL2', 'FCL3', 'FCL4', 'FCL5', 'BCL1', 'BCL2', 'BCL3', 'BCL4', 'BCL5', 'MCP1', 'MCP2', 'MCP3', 'MCP4', 'MCP5', 'FCP1', 'FCP2', 'FCP3', 'FCP4', 'FCP5', 'BCP1', 'BCP2', 'BCP3', 'BCP4', 'BCP5', 'RCI', 'INF', 'MIS', 'Graduate']\n",
    "    return df[cols]\n",
    "\n",
    "def standardize_data(df):\n",
    "    '''\n",
    "    Takes formatted data from format_data() and standardizes columns\n",
    "    Inputs: df - RASP_data object after formatting\n",
    "    Outputs: all columns standardized except graduate\n",
    "    '''\n",
    "    #remove the response variable from the data set\n",
    "    grad = df[\"Graduate\"].copy(deep = True)\n",
    "    del df[\"Graduate\"]\n",
    "    #standardize the dataframe\n",
    "    df = (df - df.mean(axis = 0))/df.std(axis = 0)\n",
    "    #add the response variable at the end\n",
    "    df[\"Graduate\"] = grad\n",
    "    return df\n",
    "\n",
    "RASP_data = standardize_data(format_data(RASP_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Bootstrapping more datapoints\n",
    "\n",
    "We decided to bootstrap data to get more observations. We did this by treating the graduated observations as one group and non-graduates as another group. Then, we simulated graduates and non-graduated by filling the columns in with a random choice (for categorical columns) or a normal distribution choice (for numerical data) based on the group distributions. This allows us to get a more representative and larger group of observations for RASP candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def get_bootstrap(runFunction = False):\n",
    "    \"\"\"\n",
    "    Allows user to run bootstrap function to bootstrap 20000 observations\n",
    "    off of given train data. If user does not want to wait 20 minutes for\n",
    "    this, the default is that it just reads a csv file where a bootstrap\n",
    "    has already been recorded\n",
    "    \"\"\"\n",
    "    from RASP_bootstrapper import bootstrap\n",
    "    if runFunction: return bootstrap(RASP_data, {'Graduate': 1})\n",
    "    return pd.read_csv(\"bootstrapped_RASP_data.csv\")\n",
    "\n",
    "bootstrapped_data = standardize_data(format_data(get_bootstrap()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Before continuing, understand that we will break our data down to 80% train and 20% test. \n",
    "\n",
    "###### We chose to use $k$ Fold Cross Validation in this project on the 80% train data in order to avoid overfitting to just this dataframe. K Fold Cross Validation is a method that subsets the data into $k$, or in this case, 5 different \"folds.\" We will then use 4 of these folds to train the model and 1 of these \"folds\" to test. We will do this repeatedly until each combination of folds in the train data has been used, saving the weights of neural network each time. We will take the mean of the weights in order to see how good our model is.\n",
    "\n",
    "###### Once we have our mean weight, we will finally test on the withheld 20% of data and observe the accuracy of the model with these mean weights. We expect this 20% to be representative of LTC Clark's data and we will not have used it in training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nacc, ws, cost, untouched_data = train_NN(bootstrapped_data, 1000, 0.8, [300, 30])\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This cell sends the data through our neural network and outputs the accuracy, weights, errors, and the data that was not used to train the neural network\n",
    "'''\n",
    "acc, ws, cost, untouched_data = train_NN(bootstrapped_data, 1000, 0.8, [300, 30])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here, we average our weights, cost, and accuracy over the 5 runs. Then, we saved our weights using pickle so we would not have to run the 90 minute training function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### Averaging\n",
    "# w, b  = ws[0]\n",
    "# w1,b1 = ws[1]\n",
    "# w2,b2 = ws[2]\n",
    "# w3,b3 = ws[3]\n",
    "# w4,b4 = ws[4]\n",
    "\n",
    "# layer1_ws = (w[0] + w1[0]+ w2[0] + w3[0] + w4[0])/5\n",
    "# layer2_ws = (w[1] + w1[1]+ w2[1] + w3[1] + w4[1])/5\n",
    "# layer3_ws = (w[2] + w1[2]+ w2[2] + w3[2] + w4[2])/5\n",
    "\n",
    "# layer1_b = (b[0] + b1[0]+ b2[0] + b3[0] + b4[0])/5\n",
    "# layer2_b = (b[1] + b1[1]+ b2[1] + b3[1] + b4[1])/5\n",
    "# layer3_b = (b[2] + b1[2]+ b2[2] + b3[2] + b4[2])/5\n",
    "\n",
    "# average_Ws = [layer1_ws,layer2_ws,layer3_ws]\n",
    "# average_b = [layer1_b,layer2_b,layer3_b]\n",
    "\n",
    "### used to save kfold results\n",
    "# with open(\"kfoldWs.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(average_Ws, f)\n",
    "\n",
    "# with open(\"kfoldBs.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(average_b, f)\n",
    "    \n",
    "# with open(\"kfoldcost.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(cost, f)\n",
    "    \n",
    "# with open(\"kfoldacc.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(acc, f)\n",
    "# with open(\"untouchedata.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(untouched_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "#importing our variables from pickle files\n",
    "with open('kfoldcost.pkl', 'rb') as f:\n",
    "    cost = pickle.load(f)\n",
    "    \n",
    "with open('kfoldacc.pkl', 'rb') as f:\n",
    "    acc = pickle.load(f)\n",
    "\n",
    "with open('kfoldBs.pkl', 'rb') as f:\n",
    "    avg_Bs = pickle.load(f)\n",
    "\n",
    "with open('kfoldWs.pkl', 'rb') as f:\n",
    "    avg_Ws = pickle.load(f)\n",
    "with open('untouchedata.pkl', 'rb') as f:\n",
    "    ut_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9072434858496446\n"
     ]
    }
   ],
   "source": [
    "#Accuracy check against untesested data from the Kfold cross validation \n",
    "ut_data = np.array(ut_data)\n",
    "Xtest = ut_data[:,0:-1].T\n",
    "Ytest = ut_data[:,-1]\n",
    "y_out, y_pred = model_predict(Xtest, avg_Ws, avg_Bs, 0.382, 4)\n",
    "acc = np.mean(y_pred==Ytest)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "#Training final model on full bootsrapped dat 80-20 train/test split. \n",
    "\n",
    "# selects = np.random.rand(len(bootstrapped_data)) < 0.8\n",
    "\n",
    "# X = np.array(bootstrapped_data[selects].iloc[:,0:-1])\n",
    "# Y = np.array(bootstrapped_data[selects].iloc[:,-1])\n",
    "# Xtest = np.array(bootstrapped_data[~selects].iloc[:,0:-1])\n",
    "# Ytest = np.array(bootstrapped_data[~selects].iloc[:,-1])\n",
    "\n",
    "\n",
    "# fWs, fbs, n, fpred_err, fcosts = train_NN_final_model(X.T, Y, 10000, 0.8, Xtest.T, Ytest, [300,30])\n",
    "\n",
    "### used to save final results\n",
    "# with open(\"FinalWs.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(fWs, f)\n",
    "\n",
    "# with open(\"FinalBs.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(fBs, f)\n",
    "    \n",
    "# with open(\"finalcost.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(fcosts, f)\n",
    "    \n",
    "# with open(\"finalerr.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(fpred_err, f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Graphing the error\n",
    "\n",
    "We graphed the error in order to ensure that over the course of fitting weights to our ANN, we see strictly decreasing error, indicating a lack of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#This code averages the error across all 5 folds and graphs it to look for potential overfitting\n",
    "def graph_avg_vals(errors):\n",
    "    \"\"\"\n",
    "    This function accepts the error vectors from train_NN and averages the errors across the multiple folds on the data. It then graphs the errors to see if there is any overfitting on the data.\n",
    "    Inputs: -errors: The error matrix (in the form of a list of lists) that contains the SSE for all the folds\n",
    "    Outputs: A graph of the average error across all the folds.\n",
    "    \"\"\"\n",
    "    y=[]\n",
    "    for i in range(len(errors[0])):\n",
    "        y.append([])\n",
    "    for idx,ycol in enumerate(errors):\n",
    "        for i,yval in enumerate(ycol):\n",
    "            y[i].append(yval)\n",
    "    for i,vals in enumerate(y):\n",
    "        y[i]=sum(vals)/len(vals)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot()\n",
    "    ax.set_ylabel(\"SSE\")\n",
    "    ax.set_xlabel(\"Iterations Through the ANN\")\n",
    "    ax.set_title(\"Graph of SSE Over Iterations Through the ANN With Kfold Cross validation\")\n",
    "    plt.plot([i for i in range(len(errors[0]))], y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "b74833e0c4cd211617e16af4c8991ff51f8c8063",
      "text/plain": "<Figure size 864x504 with 1 Axes>"
     },
     "metadata": {
      "image/png": {
       "height": 440,
       "width": 730
      },
      "needs_background": "light"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_avg_vals(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "with open(\"cost.pkl\", \"rb\") as f:\n",
    "    costss = pickle.load(f)\n",
    "    \n",
    "with open(\"pred_err.pkl\", \"rb\") as f:\n",
    "    pred_err = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "24468cd4c288d15d86786c22eb991937ae7468f3",
      "text/plain": "<Figure size 864x504 with 1 Axes>"
     },
     "metadata": {
      "image/png": {
       "height": 426,
       "width": 716
      },
      "needs_background": "light"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(costss).plot(legend=True, label=\"Sum Square Error of Training Set\")\n",
    "pd.Series(pred_err).plot(legend=True, label=\"Test Prediction Error\")\n",
    "plt.title(\"SSE of Training Set and Test Prediction Error \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# User Interface\n",
    "\n",
    "###### This is where you come in, sir. Below, we will ask you to import your data, we will format it, and our algorithm will run on it. This will consist of taking our derived weights and predicting outcomes of your data based on our weights. Then, we will tell you how accurately we were able to predict graduation rates of the people in your data based on  the model produced by our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cocalc": {
     "outputs": {
      "0": {
       "name": "input",
       "opts": {
        "password": false,
        "prompt": "Please input the string name of your data (in the form of \"csv_name.csv\"):"
       },
       "output_type": "stream"
      }
     }
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Please input the string name of your data (in the form of \"csv_name.csv\"): "
    }
   ],
   "source": [
    "def user_interface():\n",
    "    \"\"\"\n",
    "    This function assumes the data is in the local working directory of this file (in this case the data is in Project 4 folder) and is a .csv.\n",
    "    Inputs: None.\n",
    "    Outputs: Prints accuracy, returns predictions and probabilities vector.\n",
    "    \"\"\"\n",
    "    #This imports the data\n",
    "    dat_name = input('Please input the string name of your data (in the form of \"csv_name.csv\"):')\n",
    "    dat = pd.read_csv(dat_name)\n",
    "    #format and standardize the data so it works with our neural network\n",
    "    dat = format_data(dat)\n",
    "    dat = standardize_data(dat)\n",
    "    dat_np = np.array(dat.iloc[:,0:-1])\n",
    "    #Import weights (this is done because it will take too long to train the model [~90 mins]),\n",
    "    #so we already ran it and saved the weights to a pickle file. This uses our train_NN_full_modelX\n",
    "    with open(\"FullWs.pkl\", \"rb\") as f:\n",
    "        Ws = pickle.load(f)\n",
    "    with open(\"FullBs.pkl\", \"rb\") as f:\n",
    "        Bs = pickle.load(f)\n",
    "    #get predictions based on the weights. Tries two different threshhold values\n",
    "    y_prob1, y_pred1 = model_predict(dat_np.T, Ws, Bs, 0.5, 4)\n",
    "    y_prob2, y_pred2 = model_predict(dat_np.T, Ws, Bs, 0.382, 4)\n",
    "    y = np.array(dat['Graduate'])\n",
    "    #accuracy check\n",
    "    acc1 = np.mean(y_pred1==y)\n",
    "    acc2 = np.mean(y_pred2==y)\n",
    "    '''\n",
    "    if acc1 > acc2:\n",
    "        print(f'{acc1*100:.2f}%')\n",
    "        return y_pred1, y_prob1\n",
    "    '''\n",
    "    print(f'{acc2*100:.2f}%', f'{acc1*100:.2f}%')\n",
    "    \n",
    "    return acc2,acc1\n",
    "    \n",
    "results = user_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7469879518072289, 0.7521514629948365)"
      ]
     },
     "execution_count": 16,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}